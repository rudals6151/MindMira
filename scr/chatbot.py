import streamlit as st
from streamlit_chat import message
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
import pandas as pd
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

os.environ["OPENAI_API_KEY"] = "YOUR API KEY"

# ë²•ë¥  ì •ë³´ë¥¼ ìœ„í•œ ë²¡í„° DB ë¡œë“œ í•¨ìˆ˜
def load_vector_db():
    df = pd.read_csv("law_df.csv")  # ë²•ë¥  ì •ë³´ CSV íŒŒì¼
    documents = [Document(page_content=f"ì§ˆë¬¸: {row['Text']}\në‹µë³€: {row['Completion']}", metadata={"source": f"row_{i}"}) for i, row in df.iterrows()]
    
    model_name = "sentence-transformers/LaBSE"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    
    vectors = FAISS.from_documents(documents, embeddings)
    return vectors

# ë²¡í„° DB ë¡œë“œ
vector_db = load_vector_db()

# ì‹¬ë¦¬ ìƒë‹´ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("Soeon/bitaminnlp1")
model = AutoModelForCausalLM.from_pretrained("Soeon/bitaminnlp1")

# ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
sentence_model = SentenceTransformer('distilbert-base-nli-mean-tokens')

st.title('ë²•ë¥  ì •ë³´ ì œê³µ ë° ì‹¬ë¦¬ ìƒë‹´ ì±—ë´‡ğŸ˜')

# ì‚¬ìš©ì ì •ë³´ ì…ë ¥
user_name = st.text_input("ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”:")
user_age = st.number_input("ë‚˜ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", min_value=0, step=1)
user_gender = st.selectbox("ì„±ë³„ì„ ì„ íƒí•˜ì„¸ìš”:", ("ë‚¨ì", "ì—¬ì", "ì„ íƒ ì•ˆ í•¨"))

if user_name and user_age and user_gender:
    st.write(f"ì•ˆë…•í•˜ì„¸ìš”, {user_name}ë‹˜! ({user_age}ì„¸, {user_gender})")

    prompt_template = """
    ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë²•ë¥ ì— ê´€í•œ ì§ˆë¬¸ì— í•´ë‹¹ **{context}**ë¥¼ ì² ì €íˆ ê²€í† í•˜ê³ ,
    ì •í™•í•˜ê³  ëª…í™•í•œ ë²•ë¥ ì  ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”. íŠ¹íˆ, ë²•ì  ìš©ì–´ì™€ ê°œë…ì„ ì´í•´ê¸° ì‰½ë„ë¡ ìì„¸íˆ ì„¤ëª…í•˜ê³ ,
    ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ê²ƒì„ ìš°ì„ ì‹œí•´ì£¼ì„¸ìš”. 
    ìƒëŒ€ë°©ì˜ ì—°ë ¹ê³¼ ìƒí™©ì— ë§ëŠ” ë§íˆ¬ì™€ ì„¤ëª… ë°©ì‹ì„ ì‚¬ìš©í•´ ìì—°ìŠ¤ëŸ½ê³  ì¸ê°„ì ì¸ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì„¸ìš”.

    {system_prompt}

    question: {question}

    Assistant: """

    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["system_prompt", "question", "context"]
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo'),
        retriever=vector_db.as_retriever(),
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )

    def generate_prompt(age):
        if age <= 19:
            return "ì²­ì†Œë…„ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” ì¹œê·¼í•˜ê³  ì‰¬ìš´ ë§íˆ¬ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”."
        else:
            return "ì„±ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” ì •ì¤‘í•˜ê³  ëª…í™•í•œ ë§íˆ¬ë¡œ ëŒ€í™”í•´ì£¼ì„¸ìš”."

    def legal_chat(query):
        system_prompt = generate_prompt(user_age)
        result = chain(
            {
                "question": query, 
                "chat_history": st.session_state['history'],
                "system_prompt": system_prompt
            }
        )
        return result['answer']

    def psychological_chat(query):
        input_ids = tokenizer.encode(query, return_tensors="pt")
        output = model.generate(input_ids, max_length=1000, num_return_sequences=1, no_repeat_ngram_size=2)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        return response

    def get_embedding(text):
        return sentence_model.encode(text)

    def ensemble_chat(query):
        rag_response = legal_chat(query)
        ft_response = psychological_chat(query)
        
        query_embedding = get_embedding(query)
        rag_embedding = get_embedding(rag_response)
        ft_embedding = get_embedding(ft_response)
        
        rag_similarity = cosine_similarity([query_embedding], [rag_embedding])[0][0]
        ft_similarity = cosine_similarity([query_embedding], [ft_embedding])[0][0]
        
        if rag_similarity > ft_similarity:
            return rag_response
        else:
            return ft_response

    def conversational_chat(query):
        return ensemble_chat(query)

    if 'history' not in st.session_state:
        st.session_state['history'] = []

    if 'generated' not in st.session_state:
        st.session_state['generated'] = ["ì•ˆë…•í•˜ì„¸ìš”! ë²•ë¥  ë° ì‹¬ë¦¬ ìƒë‹´ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."]

    if 'past' not in st.session_state:
        st.session_state['past'] = ["ì•ˆë…•í•˜ì„¸ìš”!"]
        
    response_container = st.container()
    container = st.container()

    with container:
        with st.form(key='Conv_Question', clear_on_submit=True):           
            user_input = st.text_input("Query:", placeholder="ë²•ë¥ ì´ë‚˜ ì‹¬ë¦¬ ìƒë‹´ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.", key='input')
            submit_button = st.form_submit_button(label='Send')
            
        if submit_button and user_input:
            output = conversational_chat(user_input)
            
            st.session_state['past'].append(user_input)
            st.session_state['generated'].append(output)

    if st.session_state['generated']:
        with response_container:
            for i in range(len(st.session_state['generated'])):
                message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="fun-emoji", seed="Nala")
                message(st.session_state["generated"][i], key=str(i), avatar_style="bottts", seed="Fluffy")

else:
    st.write("ì‚¬ìš©ì ì •ë³´ë¥¼ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
